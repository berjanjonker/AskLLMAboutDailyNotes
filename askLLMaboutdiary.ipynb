{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea0cf28",
   "metadata": {},
   "source": [
    "A script to analyze Obsidian daily note files day-by-day with a local large language model (LLM)\n",
    "\n",
    "Inspired by this blog https://www.transformingmed.tech/p/decoding-happiness-an-ai-driven-experiment\n",
    "Credits for Matthijs Cluitmans - Transforming Med.Tech (http://transformingmed.tech/)\n",
    "\n",
    "Steps:\n",
    "- Install LM Studio (https://lmstudio.ai/)\n",
    "- In LM Studio, find a suitable LLM that fits with the specs of your computer. LM Studio will tell you if it will run or not. I used the Vicuna LLM (vicuna-13b-v1.5-16k.Q5_K_M.gguf) from Hugging Face https://huggingface.co/TheBloke/vicuna-13B-v1.5-16K-GGUF\n",
    "- In LM Studio, go to Local Inference Server, load the model (with the appropriate preset, e.g. Vicuna 1.5 16k), and start the server\n",
    "- Then run this Jupyter notebook with your local copy of Obsidian notes in the 'daily_notes' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f727ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Put your URI end point:port here for your local inference server (in LM Studio)\n",
    "client = OpenAI(\n",
    "    api_key='not-needed', \n",
    "    base_url='http://localhost:1234/v1')\n",
    "\n",
    "# Adjust the following based on the model type\n",
    "# Alpaca style prompt format (suitable for Vicuna):\n",
    "prefix = \"### Instruction:\\n\" \n",
    "suffix = \"\\n### Response:\"\n",
    "\n",
    "# 'Llama2 Chat' prompt format (required for some other LLMs):\n",
    "#prefix = \"[INST]\"\n",
    "#suffix = \"[/INST]\"\n",
    "\n",
    "temperature = 0 # Vary the temperature if needed; but higher temp gives more hallucinations. I had best results at 0\n",
    "\n",
    "def get_completion(prompt, messages, temperature=0.0):\n",
    "    formatted_prompt = f\"{prefix}{prompt}{suffix}\"\n",
    "    messages.extend([{\"role\": \"user\", \"content\": formatted_prompt}])\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"local model\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        presence_penalty=1.1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# These are the prompts I run on every dairy input. Adapt to your needs (e.g., now it assumes Dutch input but will respond in English.)\n",
    "systemprompt = 'You are a thorough and truthful assistant that analyses psychological content of a diary.'\n",
    "prompts = []\n",
    "prompts.append('I will give you a part of a diary in Dutch. Formatted in Markdown. Entries in the diary are seperated with \\'---\\'. We\\'re going to analyse who the user is. We\\'ll do that in a few steps. First, please extract a single \"happiness score\" from 1-10 that reflects how happy the user seems to be that day, 1 being very unhappy and 10 being very happy. Only provide number as output, nothing else, so just a single number. If you do not have sufficient information, respond with \"blank\" only and nothing else (no explanation is required).')\n",
    "prompts.append('Now, based on the diary, please provide any key takeaways, in a short sentences (in English, formatted as JSON array) related to work, family, personal development or spiritual. If no information is related to a particular category, response with \"blank\". Format the output as follows: Work: [work-related information], Family: [family-related information], Personal Development: [personal development information], Spiritual: [spiritual information].')\n",
    "prompts.append('Now, based on the diary, provide a few summarizing keywords (in English, separated by commas) that characterize the main experiences that made the user happy. Keep your response short. If you do not have sufficient information, respond \"blank\" and do not make up content that was not provided by the user. Only provide the keywords as output, nothing else. Respond in English.')\n",
    "prompts.append('Now, based on the diary, provide a few summarizing keywords (in English, separated by commas) that characterize the main experiences that made the user unhappy. Keep your response short. If you do not have sufficient information, respond \"blank\" and do not make up content that was not provided by the user. Only provide the keywords as output, nothing else. Respond in English.')\n",
    "prompts.append('Now, based on the diary, please use a few keywords (in English, separated by commas) that could help determine personality traits that are apparent from this text. If none are available (which may be likely), respond with \"blank\" only and nothing else (no explanation is required).')\n",
    "prompts.append('Now, based on the diary, please think about a few summarizing keywords (in English, separated by commas) about what is important for the user and what motivates him.Keep your response short. If you do not have sufficient information, respond \"blank\" and do not make up content that was not provided by the user. Only provide the keywords as output, nothing else. Respond in English.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ab8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection with LM Studio\n",
    "system_prompt = {\"role\": \"system\", \"content\": systemprompt}\n",
    "messages = [system_prompt]\n",
    "row = \"Ik ben vandaag zo vrolijk, zo vrolijk, zo vrolijk. Ik ben behoorlijk vrolijk, zo vrolijk was ik nooit. op mijn werk heb ik gelachen met Tom\"\n",
    "for i in range(len(prompts)):\n",
    "    if i==0:\n",
    "        prompt = prompts[0] + '\\n\\n' + row\n",
    "    else:\n",
    "        prompt = prompts[i]\n",
    "    response = get_completion(prompt, messages, temperature)\n",
    "    print(response)\n",
    "    if (response):\n",
    "        messages.extend([{\"role\": \"assistant\", \"content\": response}]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53264c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# specify the directory where the daily notes are located\n",
    "daily_notes_dir = './daily_notes/'\n",
    "\n",
    "# find all Markdown files in the directory\n",
    "markdown_files = glob.glob(os.path.join(daily_notes_dir, '*.md'))\n",
    "\n",
    "# read the output file so we can skip already processed files\n",
    "existing_output = ''\n",
    "with open('diaryLLMoutput.txt', 'r', encoding=\"utf-8\") as output:\n",
    "    existing_output = output.read();\n",
    "\n",
    "with open('diaryLLMoutput.txt', 'a', encoding=\"utf-8\") as outputfile:\n",
    "    outputfile.write('\\n\\n-----------------------------\\n NEW RUN\\n-----------------------------')\n",
    "    \n",
    "    # loop through the files and print the date and content as strings\n",
    "    for markdown_file in markdown_files:\n",
    "        system_prompt = {\"role\": \"system\", \"content\": systemprompt}\n",
    "        with open(markdown_file, 'r', encoding='utf-8') as f:\n",
    "            daily_note = f.read()\n",
    "            date = os.path.splitext(os.path.basename(markdown_file))[0]\n",
    "\n",
    "            # If you restart the process we want to skip already processed notes\n",
    "            if date in existing_output:\n",
    "                print(f'Skipping Date: {date}; already processed')\n",
    "                continue\n",
    "            \n",
    "            messages = [system_prompt] # start over again PER DAY (I also tried to retain a few days but this did not improve results considerably)\n",
    "            print(f'\\nProcessing: Date: {date}\\nContent:\\n{daily_note}\\n')\n",
    "\n",
    "            # Remark: processing takes a long time, much longer than what you're expected from cloud-based solutions such as ChatGPT. So be patient.\n",
    "            outputfile.write('\\n' + date + ';')\n",
    "            outputfile.flush()\n",
    "            for i in range(len(prompts)):\n",
    "                if i==0:\n",
    "                    prompt = prompts[0] + '\\n\\n' + daily_note\n",
    "                else:\n",
    "                    prompt = prompts[i]\n",
    "                response = get_completion(prompt, messages=messages, temperature=temperature)\n",
    "                outputfile.write(response+ ';')\n",
    "                # Write the response to the file\n",
    "                messages.extend([{\"role\": \"assistant\", \"content\": response}]) \n",
    "                outputfile.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f42fe",
   "metadata": {},
   "source": [
    "Now we do have a text file containing the output from the LLM. In my run, the data was not consistent. The script below interprets the output, executes a cleanup, and records it column by column into a CSV file.\n",
    "Please note, in my set, I had to rectify 5 rows due to the LLM generating a lot of duplicate phrases. I recommend printing a single column and scrolling through the dump to verify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8db0568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "def read_chuncks_from_file(file):\n",
    "    # Initialize the chunks list\n",
    "    chunks = []\n",
    "\n",
    "    with open(file, 'r') as input_file:\n",
    "        chunk = ''\n",
    "        for line in input_file:\n",
    "            line = line.replace('\\n','')\n",
    "            # Check if the line starts with a date\n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2}', line):\n",
    "                chunks.append(chunk)\n",
    "                chunk = line\n",
    "            else:\n",
    "                chunk = chunk + line\n",
    "    return chunks\n",
    "\n",
    "def find_happiness(text):\n",
    "    # find first integer\n",
    "    match = re.search(r'\\b\\d+\\b', text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return ''\n",
    "\n",
    "def find_takeaways(subject, text):\n",
    "    result = text.split(subject)\n",
    "        \n",
    "    if len(result) == 2:\n",
    "        match = re.search(r'\\[(.*?)\\]', result[1])\n",
    "        if match:\n",
    "            return match.group(1).replace('\"','')\n",
    "    return ''\n",
    "\n",
    "def ignore_words(text, words):\n",
    "    for word in words:\n",
    "        text = text.replace(word, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def trim(text):\n",
    "    return ignore_words(text, ['Blank', 'blank', '[', ']', '\"']).strip()\n",
    "\n",
    "# Try to delete the file\n",
    "if os.path.exists('diary.csv'):\n",
    "    os.remove('diary.csv')\n",
    "\n",
    "with open('diary.csv', 'w') as output_file:\n",
    "    output_file.write('date;happiness score;takeaways work;takeaways family;takeaways personal development;takeaways spiritual;keywords happy;keywords unhappy;personality traits;keywords motivaion')\n",
    "    \n",
    "    dates = read_chuncks_from_file('diaryLLMoutput.txt')\n",
    "    # Iterate through each date\n",
    "    for date in dates:\n",
    "\n",
    "        if len(date) == 0:\n",
    "            continue\n",
    "\n",
    "        # parse, standarize and cleanup the content\n",
    "        columns = date.split(';')\n",
    "        c_date = columns[0]\n",
    "        c_happiness = trim(find_happiness(columns[1]))\n",
    "        c_work = trim(find_takeaways('Work', columns[2]))\n",
    "        c_family = trim(find_takeaways('Family', columns[2]))\n",
    "        c_personal = trim(find_takeaways('Personal Development', columns[2]))\n",
    "        c_spiritual = trim(find_takeaways('Spiritual', columns[2]))\n",
    "        c_happy = trim(ignore_words(columns[3], ['happy', 'happiness, ', 'happy experiences: ', 'happiness-experiences=', 'happiness-keywords=', 'happiness-keywords, ']))\n",
    "        c_unhappy = trim(ignore_words(columns[4], ['unhappiness-keywords: ', 'unhappy, ', 'unhappiness, ', 'unhappiness-experiences=', 'unhappy experiences: ', 'unhappiness-keywords, ']))\n",
    "        c_traits = trim(ignore_words(columns[5], ['personality-traits=', 'personality traits: ', 'Personality traits: ', 'Personality Traits: ', 'personality-traits-keywords, ', 'personality-trait-keywords, ', 'Keywords: ', 'personality-keywords=', 'personality-keywords, ', 'personality-keywords:, ']))\n",
    "        c_motivation = trim(ignore_words(columns[6], ['importance-motivation=', 'importance-motivation: ', 'what\\'s important/motivates: ', 'importance-motivation-keywords, ', 'importance-motivation-keywords: ', 'Keywords: ', 'Important/motivation: ', 'motivation-keywords: ', 'motivation-keywords', 'importance-keywords, ', 'Important for User/Motivation: ', 'importance/motivation = ']))\n",
    "\n",
    "        # write csv row to file\n",
    "        output_file.write(';'.join([c_date, c_happiness, c_work, c_family, c_personal, c_spiritual, c_happy, c_unhappy, c_traits, c_motivation]) + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
