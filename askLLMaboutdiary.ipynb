{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea0cf28",
   "metadata": {},
   "source": [
    "A script to analyze a diary file day-by-day with a local large language model (LLM)\n",
    "\n",
    "See https://www.transformingmed.tech/p/decoding-happiness-an-ai-driven-experiment\n",
    "\n",
    "Matthijs Cluitmans for Transforming Med.Tech (http://transformingmed.tech/)\n",
    "\n",
    "Steps:\n",
    "- Install LM Studio (https://lmstudio.ai/)\n",
    "- In LM Studio, find a suitable LLM that fits with the specs of your computer. LM Studio will tell you if it will run or not. I used the Vicuna LLM (vicuna-13b-v1.5-16k.Q5_K_M.gguf) from Hugging Face https://huggingface.co/TheBloke/vicuna-13B-v1.5-16K-GGUF\n",
    "- In LM Studio, go to Local Inference Server, load the model (with the appropriate preset, e.g. Vicuna 1.5 16k), and start the server\n",
    "- Then run this Jupyter notebook with your local copy of diary.csv, which contains two columns: Date and Tekst, semicolon separated (;) and is an UTF-8 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f727ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Put your URI end point:port here for your local inference server (in LM Studio)\n",
    "client = OpenAI(\n",
    "    api_key='not-needed', \n",
    "    base_url='http://localhost:1234/v1')\n",
    "\n",
    "# Adjust the following based on the model type\n",
    "# Alpaca style prompt format (suitable for Vicuna):\n",
    "prefix = \"### Instruction:\\n\" \n",
    "suffix = \"\\n### Response:\"\n",
    "\n",
    "# 'Llama2 Chat' prompt format (required for some other LLMs):\n",
    "#prefix = \"[INST]\"\n",
    "#suffix = \"[/INST]\"\n",
    "\n",
    "temperature = 0 # Vary the temperature if needed; but higher temp gives more hallucinations. I had best results at 0\n",
    "\n",
    "def get_completion(prompt, messages, temperature=0.0):\n",
    "    formatted_prompt = f\"{prefix}{prompt}{suffix}\"\n",
    "    messages.extend([{\"role\": \"user\", \"content\": formatted_prompt}])\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"local model\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        presence_penalty=1.1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# These are the prompts I run on every dairy input. Adapt to your needs (e.g., now it assumes Dutch input but will respond in English.)\n",
    "systemprompt = 'You are a thorough and truthful assistant that analyses psychological content of a diary.'\n",
    "prompts = []\n",
    "prompts.append('I will give you a part of a diary in Dutch. Formatted in Markdown. We\\'re going to analyse who the user is. We\\'ll do that in a few steps. First, please extract a single \"happiness score\" from 1-10 that reflects how happy the user seems to be that day, 1 being very unhappy and 10 being very happy. Only provide number as output, nothing else, so just a single number. If you do not have sufficient information, respond with \"blank\" only and nothing else (no explanation is required).')\n",
    "prompts.append('Now, based on the diary, please provide any key takeaways, in a short sentences (in English, formatted as JSON array) related to work, family, personal development or spiritual. If no information is related to a particular category, response with \"blank\". Format the output as follows: Work: [work-related information], Family: [family-related information], Personal Development: [personal development information], Spiritual: [spiritual information].')\n",
    "prompts.append('Now, based on the diary, provide a few summarizing keywords (in English, separated by commas) that characterize the main experiences that made the user happy. Keep your response short. If you do not have sufficient information, respond \"blank\" and do not make up content that was not provided by the user. Only provide the keywords as output, nothing else. Respond in English.')\n",
    "prompts.append('Now, based on the diary, provide a few summarizing keywords (in English, separated by commas) that characterize the main experiences that made the user unhappy. Keep your response short. If you do not have sufficient information, respond \"blank\" and do not make up content that was not provided by the user. Only provide the keywords as output, nothing else. Respond in English.')\n",
    "prompts.append('Now, based on the diary, please use a few keywords (in English, separated by commas) that could help determine personality traits that are apparent from this text. If none are available (which may be likely), respond with \"blank\" only and nothing else (no explanation is required).')\n",
    "prompts.append('Now, based on the diary, please think about a few summarizing keywords (in English, separated by commas) about what is important for the user and what motivates him.Keep your response short. If you do not have sufficient information, respond \"blank\" and do not make up content that was not provided by the user. Only provide the keywords as output, nothing else. Respond in English.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3ab8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank\n",
      "{\n",
      " \"Work\": [\"I laughed with Tom at work\"],\n",
      " \"Family\": [],\n",
      " \"Personal Development\": [],\n",
      " \"Spiritual\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test connection with LM Studio\n",
    "system_prompt = {\"role\": \"system\", \"content\": systemprompt}\n",
    "messages = [system_prompt]\n",
    "row = \"Ik ben vandaag zo vrolijk, zo vrolijk, zo vrolijk. Ik ben behoorlijk vrolijk, zo vrolijk was ik nooit. op mijn werk heb ik gelachen met Tom\"\n",
    "for i in range(len(prompts)):\n",
    "    if i==0:\n",
    "        prompt = prompts[0] + '\\n\\n' + row\n",
    "    else:\n",
    "        prompt = prompts[i]\n",
    "    response = get_completion(prompt, messages, temperature)\n",
    "    print(response)\n",
    "    if (response):\n",
    "        messages.extend([{\"role\": \"assistant\", \"content\": response}]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Read diary, ask prompt, write output to new file\n",
    "# Create output file: I had to enforce utf-8 and fix some weird emoticons in my diary.\n",
    "with open('diaryLLMoutput.txt', 'a', encoding=\"utf-8\") as outputfile:\n",
    "    outputfile.write('\\n\\n-----------------------------\\n NEW RUN\\n-----------------------------')\n",
    "    # Opening the diary file, also as UTF-8 and with ; as delimiter\n",
    "    with open('diary.csv', newline='', encoding=\"UTF-8\") as csvfile:\n",
    "        diaryreader = csv.reader(csvfile, delimiter=';')\n",
    "        line_count = 0\n",
    "        system_prompt = {\"role\": \"system\", \"content\": systemprompt}\n",
    "        messages = [system_prompt] # initialize with system prompt\n",
    "        for row in diaryreader:\n",
    "            if line_count > 0:\n",
    "                messages = [system_prompt] # start over again PER DAY (I also tried to retain a few days but this did not improve results considerably)\n",
    "                response = []\n",
    "                print('\\nProcessing ' + row[0])\n",
    "                # Remark: processing takes a long time, much longer than what you're expected from cloud-based solutions such as ChatGPT. So be patient.\n",
    "                for i in range(len(prompts)):\n",
    "                    if i==0:\n",
    "                        prompt = prompts[0] + '\\n\\n' + row[1]\n",
    "                    else:\n",
    "                        prompt = prompts[i]\n",
    "                    # Store the response\n",
    "                    response.append(get_completion(prompt, messages=messages, temperature=temperature))\n",
    "                    messages.extend([{\"role\": \"assistant\", \"content\": response[i]}]) \n",
    "                # Write the response to the file\n",
    "                outputfile.write('\\n' + row[0] + ';')\n",
    "                for x in response:\n",
    "                    outputfile.write(x+ ';')\n",
    "                outputfile.flush()\n",
    "            line_count += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53264c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# specify the directory where the daily notes are located\n",
    "daily_notes_dir = 'daily_notes'\n",
    "\n",
    "# find all Markdown files in the directory\n",
    "markdown_files = glob.glob(os.path.join(daily_notes_dir, '*.md'))\n",
    "\n",
    "# loop through the files and print the date and content as strings\n",
    "for markdown_file in markdown_files:\n",
    "    with open(markdown_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        date = os.path.splitext(os.path.basename(markdown_file))[0]\n",
    "        print(f'Date: {date}\\nContent:\\n{content}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
